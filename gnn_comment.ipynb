{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsMaRdqiibr6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Environment Setup and Library Imports:\n",
        "The code begins with setting up the environment and importing the necessary libraries and packages. It installs required packages and imports libraries such as PyTorch, PyTorch Geometric (PyG), pandas, numpy, and more.\n",
        "\n",
        "Dataset and Preprocessing:\n",
        "\n",
        "The MovieLens 100K dataset is loaded into a DataFrame df1.\n",
        "The dataset is filtered to keep only high-rated interactions (ratings of 3 or above).\n",
        "The dataset is split into train and test sets, and user and item labels are relabeled to ensure uniqueness.\n",
        "Minibatch Sampling (data_loader function):\n",
        "\n",
        "This function samples positive and negative items for users in each minibatch.\n",
        "It takes the train DataFrame, batch size, number of users, and number of items as inputs.\n",
        "It samples positive items randomly from the user's interactions and negative items not present in their interactions.\n",
        "Edge Index Generation (train_edge_index):\n",
        "\n",
        "The edge index is generated for the bipartite graph based on user-item interactions.\n",
        "Each user is connected to the items they have interacted with, and vice versa.\n",
        "LightGCN Convolutional Layer (LightGCNConv class):\n",
        "\n",
        "The LightGCN convolutional layer is defined by inheriting from PyG's MessagePassing class.\n",
        "The forward method computes normalization factors and propagates messages.\n",
        "The message method computes the normalized messages for aggregation.\n",
        "NGCF Layer (NGCFConv class):\n",
        "\n",
        "The NGCF convolutional layer is defined similarly to the LightGCNConv class.\n",
        "It includes linear layers and activation functions for message aggregation.\n",
        "Dropout is applied to the output before the activation.\n",
        "Recommender System GNN (RecSysGNN class):\n",
        "\n",
        "This class stacks the specified number of convolutional layers (NGCF or LightGCN).\n",
        "It includes methods for forward propagation and encoding minibatches of users and items.\n",
        "Loss Function and Metrics:\n",
        "\n",
        "The Bayesian Personalized Ranking (BPR) loss function is implemented.\n",
        "The precision@K and recall@K metrics are computed for evaluation.\n",
        "Train and Evaluate Models:\n",
        "\n",
        "The train_and_eval function trains and evaluates the model for a specified number of epochs.\n",
        "It includes the training loop, computing losses, and evaluating metrics.\n",
        "Training and Evaluating LightGCN:\n",
        "\n",
        "An instance of RecSysGNN is created for LightGCN.\n",
        "The optimizer and learnable parameters are configured.\n",
        "Training and evaluation are performed, and loss and metric values are recorded.\n",
        "Training and Evaluating NGCF:\n",
        "\n",
        "Similar to LightGCN, an instance of RecSysGNN is created for NGCF.\n",
        "The optimizer and learnable parameters are configured.\n",
        "Training and evaluation are performed, and loss and metric values are recorded.\n",
        "Comparing Model Performance:\n",
        "\n",
        "The maximum precision and recall values are compared for both LightGCN and NGCF models.\n",
        "Generating Movie Recommendations:\n",
        "\n",
        "The recommend_movies function generates movie recommendations for a specific user.\n",
        "It computes the relevance scores of all items and selects the top K items.\n",
        "Output Recommendations:\n",
        "\n",
        "The recommended movies using both LightGCN and NGCF models are printed for a given user.\n",
        "Please note that while I've provided explanations for each major section, understanding the full code and its intricacies may require a deeper understanding of PyTorch, PyTorch Geometric, and recommender system techniques."
      ],
      "metadata": {
        "id": "podbVyVtinV4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1sxWXxLtixqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create LabelEncoder for user IDs\n",
        "le_user = pp.LabelEncoder()\n",
        "\n",
        "# Create LabelEncoder for item IDs\n",
        "le_item = pp.LabelEncoder()\n",
        "\n",
        "# Encode user IDs and store the indices in 'user_id_idx' column\n",
        "train_df['user_id_idx'] = le_user.fit_transform(train_df['user_id'].values)\n",
        "\n",
        "# Encode item IDs and store the indices in 'item_id_idx' column\n",
        "train_df['item_id_idx'] = le_item.fit_transform(train_df['item_id'].values)\n"
      ],
      "metadata": {
        "id": "CMBXrR5_i2Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique user IDs from the training dataframe\n",
        "train_user_ids = train_df['user_id'].unique()\n",
        "\n",
        "# Get unique item IDs from the training dataframe\n",
        "train_item_ids = train_df['item_id'].unique()\n",
        "\n",
        "# Print the number of unique user IDs and item IDs\n",
        "print(len(train_user_ids), len(train_item_ids))\n",
        "\n",
        "# Filter and update the test dataframe to keep only rows with user and item IDs present in the training data\n",
        "test_df = test_df[\n",
        "  (test_df['user_id'].isin(train_user_ids)) & \\  # Select rows where user_id is in the list of train_user_ids\n",
        "  (test_df['item_id'].isin(train_item_ids))    # Select rows where item_id is in the list of train_item_ids\n",
        "]\n",
        "\n",
        "# Print the length of the updated test dataframe\n",
        "print(len(test_df))\n"
      ],
      "metadata": {
        "id": "ZyYCEuYPjyrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform user IDs in the test dataframe to their corresponding encoded indices using le_user\n",
        "test_df['user_id_idx'] = le_user.transform(test_df['user_id'].values)\n",
        "\n",
        "# Transform item IDs in the test dataframe to their corresponding encoded indices using le_item\n",
        "test_df['item_id_idx'] = le_item.transform(test_df['item_id'].values)\n"
      ],
      "metadata": {
        "id": "sihSOxC-j5uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique user indices in the training data\n",
        "n_users = train_df['user_id_idx'].nunique()\n",
        "\n",
        "# Calculate the number of unique item indices in the training data\n",
        "n_items = train_df['item_id_idx'].nunique()\n",
        "\n",
        "# Print the number of unique users\n",
        "print(\"Number of Unique Users: \", n_users)\n",
        "\n",
        "# Print the number of unique items\n",
        "print(\"Number of Unique Items: \", n_items)\n"
      ],
      "metadata": {
        "id": "msotCr5mj_Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for loading data in batches\n",
        "def data_loader(data, batch_size, n_usr, n_itm):\n",
        "\n",
        "    # Define a function to sample negative items\n",
        "    def sample_neg(x):\n",
        "        while True:\n",
        "            # Generate a random negative item index\n",
        "            neg_id = random.randint(0, n_itm - 1)\n",
        "            # Check if the negative item index is not in the list of positive items\n",
        "            if neg_id not in x:\n",
        "                return neg_id\n",
        "\n",
        "    # Group the data by user indices and create a list of interacted item indices for each user\n",
        "    interected_items_df = data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
        "\n",
        "    # Create a list of indices representing users\n",
        "    indices = [x for x in range(n_usr)]\n",
        "\n",
        "    # If the number of users is less than the batch size, select users randomly (with replacement)\n",
        "    if n_usr < batch_size:\n",
        "        users = [random.choice(indices) for _ in range(batch_size)]\n",
        "    # If the number of users is greater than or equal to the batch size, sample users without replacement\n",
        "    else:\n",
        "        users = random.sample(indices, batch_size)\n",
        "    users.sort()\n",
        "\n",
        "    # Create a DataFrame with the selected user indices\n",
        "    users_df = pd.DataFrame(users, columns=['users'])\n",
        "\n",
        "    # Merge the interacted_items_df with the users_df to filter only the selected users' data\n",
        "    interected_items_df = pd.merge(interected_items_df, users_df, how='right', left_on='user_id_idx', right_on='users')\n",
        "\n",
        "    # Sample positive items for each selected user randomly from their interacted items\n",
        "    pos_items = interected_items_df['item_id_idx'].apply(lambda x: random.choice(x)).values\n",
        "\n",
        "    # Sample negative items for each selected user using the sample_neg function\n",
        "    neg_items = interected_items_df['item_id_idx'].apply(lambda x: sample_neg(x)).values\n",
        "\n",
        "    # Return the selected users, positive item indices, and negative item indices as PyTorch LongTensors\n",
        "    return (\n",
        "        torch.LongTensor(list(users)).to(device),                  # Selected user indices\n"
      ],
      "metadata": {
        "id": "LVr0IhuDkAMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert user indices and adjusted item indices to PyTorch LongTensors\n",
        "u_t = torch.LongTensor(train_df.user_id_idx)             # User indices\n",
        "i_t = torch.LongTensor(train_df.item_id_idx) + n_users   # Adjusted item indices (adding n_users to separate from users)\n",
        "\n",
        "# Create the training edge index tensor by stacking user indices and item indices\n",
        "# The edge index represents connections between nodes (users and items) in a graph\n",
        "train_edge_index = torch.stack((\n",
        "    torch.cat([u_t, i_t]),   # Concatenate user indices and adjusted item indices for the \"from\" end of edges\n",
        "    torch.cat([i_t, u_t])    # Concatenate adjusted item indices and user indices for the \"to\" end of edges\n",
        ")).to(device)\n",
        "\n",
        "# Print the resulting training edge index tensor\n",
        "print(train_edge_index)\n"
      ],
      "metadata": {
        "id": "5KXUh1qckJWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the last column of the train_edge_index tensor\n",
        "destination_nodes = train_edge_index[:, -1]  # Extracting the destination (target) nodes (items)\n",
        "\n",
        "# Extract the first column of the train_edge_index tensor\n",
        "source_nodes = train_edge_index[:, 0]        # Extracting the source nodes (users)\n"
      ],
      "metadata": {
        "id": "QgdrS9RykTV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index = 0  # Index of the edge you want to retrieve\n",
        "source_node_index = train_edge_index[:, edge_index][0]  # Index of the source node (user) of the edge\n",
        "destination_node_index = train_edge_index[:, edge_index][1]  # Index of the destination node (item) of the edge\n"
      ],
      "metadata": {
        "id": "PtuV971YkY3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "It looks like you have created a tensor train_edge_index that represents the edge indices of a graph. The first row of the tensor contains the source node indices (users), and the second row contains the destination node indices (items) for each edge. The output you provided shows that the tensor contains two edges.\n",
        "\n",
        "Let's break down the outputs you've provided:\n",
        "\n",
        "train_edge_index[:,-1], train_edge_index[:, 0]: This extracts the last node index of the source nodes and the first node index of the destination nodes from the train_edge_index tensor. This output indicates that the last edge's source node index is 477 and the first edge's destination node index is 64.\n",
        "\n",
        "train_edge_index[:, len(train)-1], train_edge_index[:, len(train)]: This seems to be a misunderstanding. The len(train)-1 index is used to extract the last column (edge) from the train_edge_index tensor, while len(train) is an invalid index as it's one greater than the last valid index.\n",
        "\n",
        "If you want to extract specific edges from the train_edge_index tensor, you should provide valid indices for the second dimension (edge index). For example:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "edge_index = 0  # Index of the edge you want to retrieve\n",
        "source_node_index = train_edge_index[0, edge_index]  # Index of the source node (user) of the edge\n",
        "destination_node_index = train_edge_index[1, edge_index]  # Index of the destination node (item) of the edge\n",
        "In your case, train_edge_index[:, -1] and train_edge_index[:, 0] give you the source and destination node indices for the last and first edges, respectively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YLmbhBRTkoDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The code defines a custom graph convolutional layer called LightGCNConv using the PyTorch Geometric library. This layer inherits from MessagePassing, and it performs message passing on a graph using a simplified version of the LightGCN algorithm.\n",
        "\n",
        "The key steps in the code are as follows:\n",
        "\n",
        "Class Definition: Define the LightGCNConv class inheriting from MessagePassing.\n",
        "\n",
        "Constructor: Initialize the class by calling the parent class's constructor (super().__init__(aggr='add')).\n",
        "\n",
        "Forward Method: Define the forward pass of the layer. It takes input node features (x) and edge indices (edge_index) as inputs.\n",
        "\n",
        "Normalization Calculation: Calculate the degree of each destination node and compute the inverse square root of these degrees for normalization. Handle cases where the degree is infinity by setting it to 0.\n",
        "\n",
        "Message Passing: Use the inherited propagate method to perform message passing. This method requires specifying the edge_index, input node features (x), and the computed normalization (norm).\n",
        "\n",
        "Message Method: Define the message computation. Multiply the normalized values (norm) with the source node features (x_j) to scale the features based on the degree-based normalization.\n",
        "\n",
        "This custom layer can be used as a building block in graph convolutional neural network architectures that follow the LightGCN algorithm. The key idea behind LightGCN is to simplify the message passing step by only considering the degree-based normalization and omitting the use of activation functions and biases.\n",
        "\n",
        "Remember that this layer is part of a larger framework and is used alongside other components like data loading, model training loops, and evaluation metrics to create a complete recommendation system based on graph neural networks.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uzPRBatQkpU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "# Define a custom graph convolutional layer class\n",
        "class LightGCNConv(MessagePassing):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(aggr='add')  # Call parent class constructor with 'add' aggregation method\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Compute normalization\n",
        "        from_, to_ = edge_index  # Split edge_index into source and destination nodes\n",
        "        deg = degree(to_, x.size(0), dtype=x.dtype)  # Calculate degree of each destination node\n",
        "        deg_inv_sqrt = deg.pow(-0.5)  # Calculate inverse square root of degrees for normalization\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0  # Handle infinity degrees by setting to 0\n",
        "        norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]  # Compute normalization values\n",
        "\n",
        "        # Start propagating messages (no update after aggregation)\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        # Multiply normalized values with source node features for message computation\n",
        "        return norm.view(-1, 1) * x_j\n"
      ],
      "metadata": {
        "id": "ujaEfPm3k2H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "# Define a custom graph convolutional layer class\n",
        "class LightGCNConv(MessagePassing):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(aggr='add')\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        from_, to_ = edge_index\n",
        "        deg = degree(to_, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "# Create a test input tensor with shape (5, 5) representing an identity matrix\n",
        "test_x = torch.Tensor(np.eye(5))\n",
        "\n",
        "# Create a test edge index tensor with shape (2, 8) representing source and destination nodes\n",
        "test_edge_index = torch.LongTensor(np.array([\n",
        "  [0, 0, 1, 1, 2, 3, 3, 4],\n",
        "  [2, 3, 3, 4, 0, 0, 1, 1]\n",
        "]))\n",
        "\n",
        "# Instantiate the LightGCNConv layer and apply it to the test input\n",
        "output = LightGCNConv()(test_x, test_edge_index)\n",
        "\n",
        "# Print the output tensor\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "GlXttGxulIGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The code initializes a test input tensor representing an identity matrix and a test edge index tensor representing source and destination nodes. Then, it creates an instance of the LightGCNConv layer and applies it to the test input data using the provided edge index. Finally, it prints the output tensor after the layer's operation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "User\n",
        "easier way\n",
        "ChatGPT\n",
        "Certainly, here's the code with comments to explain each step:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "# Define a custom graph convolutional layer class\n",
        "class LightGCNConv(MessagePassing):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(aggr='add')\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        from_, to_ = edge_index\n",
        "        deg = degree(to_, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "# Create a test input tensor (identity matrix)\n",
        "test_x = torch.Tensor(np.eye(5))\n",
        "\n",
        "# Create a test edge index tensor (source and destination nodes)\n",
        "test_edge_index = torch.LongTensor(np.array([\n",
        "  [0, 0, 1, 1, 2, 3, 3, 4],\n",
        "  [2, 3, 3, 4, 0, 0, 1, 1]\n",
        "]))\n",
        "\n",
        "# Instantiate the LightGCNConv layer\n",
        "lightgcn_layer = LightGCNConv()\n",
        "\n",
        "# Apply the layer to the test input\n",
        "output = lightgcn_layer(test_x, test_edge_index)\n",
        "\n",
        "# Print the output\n",
        "print(output)\n",
        "This code creates the LightGCNConv layer instance, applies it to the test input data, and prints the output, following the same steps as the previous example but with added comments for clarity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "56g-slePlP4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pABFmVGzlMXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "# Define a graph convolutional layer class called NGCFConv\n",
        "class NGCFConv(MessagePassing):\n",
        "    def __init__(self, latent_dim, dropout, bias=True, **kwargs):\n",
        "        super(NGCFConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        # Store dropout rate\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define two linear layers for message transformation\n",
        "        self.lin_1 = nn.Linear(latent_dim, latent_dim, bias=bias)\n",
        "        self.lin_2 = nn.Linear(latent_dim, latent_dim, bias=bias)\n",
        "\n",
        "        # Initialize layer parameters\n",
        "        self.init_parameters()\n",
        "\n",
        "    # Initialize linear layer parameters using Xavier initialization\n",
        "    def init_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin_1.weight)\n",
        "        nn.init.xavier_uniform_(self.lin_2.weight)\n",
        "\n",
        "    # Define the forward pass of the NGCFConv layer\n",
        "    def forward(self, x, edge_index):\n",
        "        # Compute normalization coefficients\n",
        "        from_, to_ = edge_index\n",
        "        deg = degree(to_, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
        "\n",
        "        # Start message propagation and aggregation\n",
        "        out = self.propagate(edge_index, x=(x, x), norm=norm)\n",
        "\n",
        "        # Update nodes after aggregation\n",
        "        out += self.lin_1(x)\n",
        "        out = F.dropout(out, self.dropout, self.training)\n",
        "        return F.leaky_relu(out)\n",
        "\n",
        "    # Define the message computation for each node\n",
        "    def message(self, x_j, x_i, norm):\n",
        "        # Transform and combine neighbor node features\n",
        "        return norm.view(-1, 1) * (self.lin_1(x_j) + self.lin_2(x_j * x_i))\n",
        "\n",
        "# Code for graph convolutional layer (NGCFConv) is defined above\n",
        "\n",
        "# Commented explanation:\n",
        "# This code defines a graph convolutional layer called NGCFConv. It inherits from the MessagePassing class in PyTorch Geometric\n",
        "# and implements a neighborhood-based collaborative filtering approach.\n",
        "# The layer takes a latent dimension, dropout rate, and bias as inputs.\n",
        "# It initializes two linear transformation layers for message passing.\n",
        "# The init_parameters method initializes the layer parameters using Xavier uniform initialization.\n",
        "# The forward method computes the normalization coefficients for message propagation.\n",
        "# It propagates messages and aggregates them using the specified aggregation method.\n",
        "# The node features are updated after aggregation using the lin_1 layer and dropout.\n",
        "# The message method computes messages by transforming and combining neighbor node features using lin_1 and lin_2 layers.\n"
      ],
      "metadata": {
        "id": "lPjlho5alf__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "# Define a custom graph convolutional layer for NGCF model\n",
        "class NGCFConv(MessagePassing):\n",
        "    def __init__(self, latent_dim, dropout, bias=True, **kwargs):\n",
        "        super(NGCFConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define linear layers for message passing\n",
        "        self.lin_1 = nn.Linear(latent_dim, latent_dim, bias=bias)\n",
        "        self.lin_2 = nn.Linear(latent_dim, latent_dim, bias=bias)\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.init_parameters()\n",
        "\n",
        "    # Initialize linear layer parameters using Xavier initialization\n",
        "    def init_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin_1.weight)\n",
        "        nn.init.xavier_uniform_(self.lin_2.weight)\n",
        "\n",
        "    # Define the forward pass for NGCFConv\n",
        "    def forward(self, x, edge_index):\n",
        "        # Compute normalization factors\n",
        "        from_, to_ = edge_index\n",
        "        deg = degree(to_, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
        "\n",
        "        # Start propagating messages\n",
        "        out = self.propagate(edge_index, x=(x, x), norm=norm)\n",
        "\n",
        "        # Perform update after aggregation\n",
        "        out += self.lin_1(x)\n",
        "        out = F.dropout(out, self.dropout, self.training)\n",
        "        return F.leaky_relu(out)\n",
        "\n",
        "    # Define the message function for NGCFConv\n",
        "    def message(self, x_j, x_i, norm):\n",
        "        return norm.view(-1, 1) * (self.lin_1(x_j) + self.lin_2(x_j * x_i))\n",
        "\n",
        "# Define a custom graph neural network model for recommendation\n",
        "class RecSysGNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_dim,\n",
        "        num_layers,\n",
        "        num_users,\n",
        "        num_items,\n",
        "        model,   # Model type: 'NGCF' or 'LightGCN'\n",
        "        dropout=0.1  # Dropout rate (only used in NGCF)\n",
        "    ):\n",
        "        super(RecSysGNN, self).__init__()\n",
        "\n",
        "        # Check if the model is valid\n",
        "        assert (model == 'NGCF' or model == 'LightGCN'), \\\n",
        "            'Model must be NGCF or LightGCN'\n",
        "        self.model = model\n",
        "\n",
        "        # Create an embedding layer for users and items\n",
        "        self.embedding = nn.Embedding(num_users + num_items, latent_dim)\n",
        "\n",
        "        # Initialize convolutional layers based on the chosen model\n",
        "        if self.model == 'NGCF':\n",
        "            self.convs = nn.ModuleList(\n",
        "                NGCFConv(latent_dim, dropout=dropout) for _ in range(num_layers)\n",
        "            )\n",
        "        else:\n",
        "            self.convs = nn.ModuleList(LightGCNConv() for _ in range(num_layers))\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.init_parameters()\n",
        "\n",
        "    # Initialize embedding and convolutional layer parameters\n",
        "    def init_parameters(self):\n",
        "        if self.model == 'NGCF':\n",
        "            nn.init.xavier_uniform_(self.embedding.weight, gain=1)\n",
        "        else:\n",
        "            nn.init.normal_(self.embedding.weight, std=0.1)\n",
        "\n",
        "    # Define the forward pass of the model\n",
        "    def forward(self, edge_index):\n",
        "        emb0 = self.embedding.weight\n",
        "        embs = [emb0]\n",
        "\n",
        "        emb = emb0\n",
        "        for conv in self.convs:\n",
        "            emb = conv(x=emb, edge_index=edge_index)\n",
        "            embs.append(emb)\n",
        "\n",
        "        out = (\n",
        "            torch.cat(embs, dim=-1) if self.model == 'NGCF'\n",
        "            else torch.mean(torch.stack(embs, dim=0), dim=0)\n",
        "        )\n",
        "\n",
        "        return emb0, out\n",
        "\n",
        "    # Encode a minibatch of data\n",
        "    def encode_minibatch(self, users, pos_items, neg_items, edge_index):\n",
        "        emb0, out = self(edge_index)\n",
        "        return (\n",
        "            out[users],\n",
        "            out[pos_items],\n",
        "            out[neg_items],\n",
        "            emb0[users],\n",
        "            emb0[pos_items],\n",
        "            emb0[neg_items]\n",
        "        )\n"
      ],
      "metadata": {
        "id": "NUADGD0glg13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8BT4ZIzl8N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Here's the code with comments explaining each part:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "# Define a custom graph convolutional layer for NGCF model\n",
        "class NGCFConv(MessagePassing):\n",
        "    def __init__(self, latent_dim, dropout, bias=True, **kwargs):\n",
        "        super(NGCFConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define linear layers for message passing\n",
        "        self.lin_1 = nn.Linear(latent_dim, latent_dim, bias=bias)\n",
        "        self.lin_2 = nn.Linear(latent_dim, latent_dim, bias=bias)\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.init_parameters()\n",
        "\n",
        "    # Initialize linear layer parameters using Xavier initialization\n",
        "    def init_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin_1.weight)\n",
        "        nn.init.xavier_uniform_(self.lin_2.weight)\n",
        "\n",
        "    # Define the forward pass for NGCFConv\n",
        "    def forward(self, x, edge_index):\n",
        "        # Compute normalization factors\n",
        "        from_, to_ = edge_index\n",
        "        deg = degree(to_, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
        "\n",
        "        # Start propagating messages\n",
        "        out = self.propagate(edge_index, x=(x, x), norm=norm)\n",
        "\n",
        "        # Perform update after aggregation\n",
        "        out += self.lin_1(x)\n",
        "        out = F.dropout(out, self.dropout, self.training)\n",
        "        return F.leaky_relu(out)\n",
        "\n",
        "    # Define the message function for NGCFConv\n",
        "    def message(self, x_j, x_i, norm):\n",
        "        return norm.view(-1, 1) * (self.lin_1(x_j) + self.lin_2(x_j * x_i))\n",
        "\n",
        "# Define a custom graph neural network model for recommendation\n",
        "class RecSysGNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_dim,\n",
        "        num_layers,\n",
        "        num_users,\n",
        "        num_items,\n",
        "        model,   # Model type: 'NGCF' or 'LightGCN'\n",
        "        dropout=0.1  # Dropout rate (only used in NGCF)\n",
        "    ):\n",
        "        super(RecSysGNN, self).__init__()\n",
        "\n",
        "        # Check if the model is valid\n",
        "        assert (model == 'NGCF' or model == 'LightGCN'), \\\n",
        "            'Model must be NGCF or LightGCN'\n",
        "        self.model = model\n",
        "        \n",
        "        # Create an embedding layer for users and items\n",
        "        self.embedding = nn.Embedding(num_users + num_items, latent_dim)\n",
        "\n",
        "        # Initialize convolutional layers based on the chosen model\n",
        "        if self.model == 'NGCF':\n",
        "            self.convs = nn.ModuleList(\n",
        "                NGCFConv(latent_dim, dropout=dropout) for _ in range(num_layers)\n",
        "            )\n",
        "        else:\n",
        "            self.convs = nn.ModuleList(LightGCNConv() for _ in range(num_layers))\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.init_parameters()\n",
        "\n",
        "    # Initialize embedding and convolutional layer parameters\n",
        "    def init_parameters(self):\n",
        "        if self.model == 'NGCF':\n",
        "            nn.init.xavier_uniform_(self.embedding.weight, gain=1)\n",
        "        else:\n",
        "            nn.init.normal_(self.embedding.weight, std=0.1)\n",
        "\n",
        "    # Define the forward pass of the model\n",
        "    def forward(self, edge_index):\n",
        "        emb0 = self.embedding.weight\n",
        "        embs = [emb0]\n",
        "\n",
        "        emb = emb0\n",
        "        for conv in self.convs:\n",
        "            emb = conv(x=emb, edge_index=edge_index)\n",
        "            embs.append(emb)\n",
        "\n",
        "        out = (\n",
        "            torch.cat(embs, dim=-1) if self.model == 'NGCF'\n",
        "            else torch.mean(torch.stack(embs, dim=0), dim=0)\n",
        "        )\n",
        "\n",
        "        return emb0, out\n",
        "\n",
        "    # Encode a minibatch of data\n",
        "    def encode_minibatch(self, users, pos_items, neg_items, edge_index):\n",
        "        emb0, out = self(edge_index)\n",
        "        return (\n",
        "            out[users],\n",
        "            out[pos_items],\n",
        "            out[neg_items],\n",
        "            emb0[users],\n",
        "            emb0[pos_items],\n",
        "            emb0[neg_items]\n",
        "        )\n",
        "This code defines a recommendation system using graph neural networks. It includes a custom graph convolutional layer (NGCFConv) for the NGCF model, and a main model class (RecSysGNN) that can be configured to use either NGCF or LightGCN. The code sets up the layers and methods needed for message propagation and parameter initialization, allowing for efficient training and encoding of minibatches of user-item interactions. The model can generate embeddings for users, items, and their interactions, making it suitable for personalized recommendations."
      ],
      "metadata": {
        "id": "btSohK3yl99S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "This code snippet defines a function compute_bpr_loss that computes the Bayesian Personalized Ranking (BPR) loss for a single minibatch of users, positive items, and negative items. The BPR loss is a common loss function used in recommendation systems that aims to optimize the ranking of positive items over negative items for each user. The loss function also includes a regularization term for the initial embeddings.\n",
        "\n",
        "Here's a breakdown of the code:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "def compute_bpr_loss(users, users_emb, pos_emb, neg_emb, user_emb0, pos_emb0, neg_emb0):\n",
        "    # Compute regularization loss from initial embeddings\n",
        "    reg_loss = (1 / 2) * (\n",
        "        user_emb0.norm().pow(2) +\n",
        "        pos_emb0.norm().pow(2) +\n",
        "        neg_emb0.norm().pow(2)\n",
        "    ) / float(len(users))\n",
        "\n",
        "    # Compute BPR loss from user, positive item, and negative item embeddings\n",
        "    pos_scores = torch.mul(users_emb, pos_emb).sum(dim=1)\n",
        "    neg_scores = torch.mul(users_emb, neg_emb).sum(dim=1)\n",
        "\n",
        "    # Compute BPR loss using softplus function\n",
        "    bpr_loss = torch.mean(F.softplus(neg_scores - pos_scores))\n",
        "\n",
        "    return bpr_loss, reg_loss\n",
        "In summary, this function computes two parts of the loss:\n",
        "\n",
        "Regularization Loss (reg_loss): It computes the L2-norm of the initial embeddings (user_emb0, pos_emb0, neg_emb0) and averages them. This term helps prevent overfitting and encourages the embeddings to stay close to the origin.\n",
        "\n",
        "BPR Loss (bpr_loss): It calculates the BPR loss using the formula for the softplus function applied to the difference between the scores of negative and positive items for each user. This loss aims to maximize the difference in scores between positive and negative items.\n",
        "\n",
        "The function returns both the BPR loss and the regularization loss, which can then be used to compute the overall loss for training the recommendation system. This loss function is commonly used in recommendation systems to optimize the ranking of items for each user based on their preferences."
      ],
      "metadata": {
        "id": "b2ixQ6mSmBH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hgd5l7qRrVq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The get_metrics function provided appears to calculate the evaluation metrics (Recall@K and Precision@K) for the movie recommendation system based on the provided user and item embeddings, training and test data, and the value of K (number of recommendations to consider). Let's go through the code step by step to understand what it's doing:\n",
        "\n",
        "Compute Relevance Scores: The function starts by computing the relevance scores for all user-item pairs using the provided user and item embeddings. This is done by matrix multiplication of the user embeddings with the transpose of the item embeddings.\n",
        "\n",
        "Create Dense Interactions Tensor: It creates a dense tensor interactions_t representing user-item interactions based on the training data. This tensor is created by stacking user indices and item indices from the training data and assigning a value of 1 to each interaction.\n",
        "\n",
        "Mask Out Training Interactions: The relevance scores are then multiplied element-wise with the complement of interactions_t. This effectively masks out the training interactions from the relevance scores, so that they won't affect the evaluation metrics.\n",
        "\n",
        "Top-K Scoring Items for Each User: For each user, the code computes the indices of the top K items with the highest relevance scores. These indices represent the recommended items for each user.\n",
        "\n",
        "Create DataFrames for Results: The indices of the top K items are converted into a DataFrame topk_relevance_indices_df which includes user IDs and the indices of top-K relevant items.\n",
        "\n",
        "Measure Overlap and Calculate Metrics: The code then calculates the overlap between the recommended items and the held-out (test) user-item interactions. It does this by comparing the recommended item indices with the actual test item indices for each user. The metrics_df DataFrame is constructed to store this information.\n",
        "\n",
        "Calculate Recall and Precision: The recall and precision are calculated for each user based on the number of overlapping items between recommended items and test items. Recall is the ratio of overlapping items to all test items, and precision is the ratio of overlapping items to K (number of recommendations).\n",
        "\n",
        "Calculate Mean Metrics: Finally, the function returns the average recall and an undefined variable metric. It seems there might be a typo in the return statement (metric instead of metrics_df).\n",
        "\n",
        "It's important to note that the evaluation metrics (recall and precision) are calculated for each user, and the overall metrics are reported as the mean values across all users in the test set. These metrics provide insight into how well the recommendation system is performing in terms of capturing relevant items for users based on the provided embeddings and the chosen value of K.\n",
        "\n",
        "However, the code has a small issue with the return statement, which needs to be fixed to properly return the calculated metrics DataFrame. Also, the provided code snippet is missing the import statements and variable definitions, so make sure you have all the required imports and variable assignments in place for the code to run successfully.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Regenerate\n"
      ],
      "metadata": {
        "id": "8DVZoIuCrXcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(user_Embed_wts, item_Embed_wts, n_users, n_items, train_data, test_data, K):\n",
        "    # Get unique user IDs from the test data\n",
        "    test_user_ids = torch.LongTensor(test_data['user_id_idx'].unique())\n",
        "\n",
        "    # Compute the relevance score of all user-item pairs using matrix multiplication\n",
        "    relevance_score = torch.matmul(user_Embed_wts, torch.transpose(item_Embed_wts, 0, 1))\n",
        "\n",
        "    # Create a dense tensor representing all user-item interactions from the training data\n",
        "    i = torch.stack((\n",
        "        torch.LongTensor(train_data['user_id_idx'].values),\n",
        "        torch.LongTensor(train_data['item_id_idx'].values)\n",
        "    ))\n",
        "    v = torch.ones((len(train_data)), dtype=torch.float64)\n",
        "    interactions_t = torch.sparse.FloatTensor(i, v, (n_users, n_items)).to_dense().to(device)\n",
        "\n",
        "    # Mask out training user-item interactions from metric computation\n",
        "    relevance_score = torch.mul(relevance_score, (1 - interactions_t))\n",
        "\n",
        "    # Compute the indices of top-scoring items for each user\n",
        "    topk_relevance_indices = torch.topk(relevance_score, K).indices\n",
        "    topk_relevance_indices_df = pd.DataFrame(topk_relevance_indices.cpu().numpy(), columns=['top_indx_'+str(x+1) for x in range(K)])\n",
        "    topk_relevance_indices_df['user_ID'] = topk_relevance_indices_df.index\n",
        "    topk_relevance_indices_df['top_rlvnt_itm'] = topk_relevance_indices_df[['top_indx_'+str(x+1) for x in range(K)]].values.tolist()\n",
        "    topk_relevance_indices_df = topk_relevance_indices_df[['user_ID', 'top_rlvnt_itm']]\n",
        "\n",
        "    # Group test data by user and store their interacted items in a DataFrame\n",
        "    test_interacted_items = test_data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
        "\n",
        "    # Merge the top-k recommended items DataFrame with the test interacted items DataFrame\n",
        "    metrics_df = pd.merge(test_interacted_items, topk_relevance_indices_df, how='left', left_on='user_id_idx', right_on=['user_ID'])\n",
        "\n",
        "    # Find the intersection of recommended and test interacted items for each user\n",
        "    metrics_df['intrsctn_itm'] = [list(set(a).intersection(b)) for a, b in zip(metrics_df.item_id_idx, metrics_df.top_rlvnt_itm)]\n",
        "\n",
        "    # Calculate recall and precision for each user\n",
        "    metrics_df['recall'] = metrics_df.apply(lambda x: len(x['intrsctn_itm']) / len(x['item_id_idx']), axis=1)\n",
        "    metrics_df['precision'] = metrics_df.apply(lambda x: len(x['intrsctn_itm']) / K, axis=1)\n",
        "\n",
        "    # Calculate the mean recall across all users and return the metrics DataFrame\n",
        "    return metrics_df['recall'].mean(), metrics_df['precision'].mean()\n"
      ],
      "metadata": {
        "id": "SwVPyG-SrYb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "It looks like this code defines a training and evaluation loop for a recommender system model using Bayesian Personalized Ranking (BPR) loss. Let's go through the code step by step and provide explanations for each part:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "latent_dim = 64\n",
        "n_layers = 3\n",
        "\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 1024\n",
        "DECAY = 0.0001\n",
        "LR = 0.005\n",
        "K = 20\n",
        "Here, several hyperparameters are defined:\n",
        "\n",
        "latent_dim: The dimensionality of the learned embeddings for users and items.\n",
        "n_layers: The number of layers in the neural network model.\n",
        "EPOCHS: The number of training epochs.\n",
        "BATCH_SIZE: The batch size used during training.\n",
        "DECAY: The weight decay factor used for regularization.\n",
        "LR: The learning rate of the optimizer.\n",
        "K: The number of top items to consider for evaluation metrics like recall and precision.\n",
        "python\n",
        "Copy code\n",
        "def train_and_eval(model, optimizer, train_df):\n",
        "    # Lists to store different types of losses and evaluation metrics for each epoch\n",
        "    loss_list_epoch = []\n",
        "    bpr_loss_list_epoch = []\n",
        "    reg_loss_list_epoch = []\n",
        "    recall_list = []\n",
        "    precision_list = []\n",
        "\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "        n_batch = int(len(train_df) / BATCH_SIZE)\n",
        "\n",
        "        final_loss_list = []\n",
        "        bpr_loss_list = []\n",
        "        reg_loss_list = []\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx in range(n_batch):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Sample users, positive items, and negative items for the batch\n",
        "            users, pos_items, neg_items = data_loader(train_df, BATCH_SIZE, n_users, n_items)\n",
        "\n",
        "            # Encode the minibatch to obtain user and item embeddings\n",
        "            users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0 = model.encode_minibatch(users, pos_items, neg_items, train_edge_index)\n",
        "\n",
        "            # Compute BPR loss and regularization loss\n",
        "            bpr_loss, reg_loss = compute_bpr_loss(\n",
        "                users, users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0\n",
        "            )\n",
        "            reg_loss = DECAY * reg_loss\n",
        "            final_loss = bpr_loss + reg_loss\n",
        "\n",
        "            # Backpropagate and update the model parameters\n",
        "            final_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Append different types of losses to lists\n",
        "            final_loss_list.append(final_loss.item())\n",
        "            bpr_loss_list.append(bpr_loss.item())\n",
        "            reg_loss_list.append(reg_loss.item())\n",
        "\n",
        "        # Evaluate the model on the test data\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            _, out = model(train_edge_index)\n",
        "            final_user_Embed, final_item_Embed = torch.split(out, (n_users, n_items))\n",
        "            test_topK_recall,  test_topK_precision = get_metrics(\n",
        "                final_user_Embed, final_item_Embed, n_users, n_items, train_df, test_df, K\n",
        "            )\n",
        "\n",
        "        # Append losses and evaluation metrics for the current epoch\n",
        "        loss_list_epoch.append(round(np.mean(final_loss_list), 4))\n",
        "        bpr_loss_list_epoch.append(round(np.mean(bpr_loss_list), 4))\n",
        "        reg_loss_list_epoch.append(round(np.mean(reg_loss_list), 4))\n",
        "        recall_list.append(round(test_topK_recall, 4))\n",
        "        precision_list.append(round(test_topK_precision, 4))\n",
        "\n",
        "    # Return the lists of losses and evaluation metrics for all epochs\n",
        "    return (\n",
        "        loss_list_epoch,\n",
        "        bpr_loss_list_epoch,\n",
        "        reg_loss_list_epoch,\n",
        "        recall_list,\n",
        "        precision_list\n",
        "    )\n",
        "In summary, the train_and_eval function trains a recommender system model for a specified number of epochs and performs evaluation after each epoch. During training, the model is trained using BPR loss, which aims to maximize the margin between the scores of positive and negative items for each user. The model is then evaluated on the test data using the get_metrics function to compute recall and precision metrics for the top-K recommendations.\n",
        "\n",
        "This code is part of a larger training and evaluation pipeline for a recommender system model. It assumes that other functions like data_loader, compute_bpr_loss, and get_metrics are defined elsewhere in the codebase and are used to handle data loading, loss computation, and evaluation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Regenerate\n"
      ],
      "metadata": {
        "id": "mdrHb-OorhzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t looks like you are training and evaluating a LightGCN model for a recommendation task. The code snippet you provided is creating an instance of the RecSysGNN model with the \"LightGCN\" architecture, moving it to the specified device (presumably GPU), setting up the optimizer, and then training and evaluating the model using the train_and_eval function. Let's break down the steps:\n",
        "\n",
        "Model Initialization:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "lightgcn = RecSysGNN(\n",
        "  latent_dim=latent_dim,\n",
        "  num_layers=n_layers,\n",
        "  num_users=n_users,\n",
        "  num_items=n_items,\n",
        "  model='LightGCN'\n",
        ")\n",
        "lightgcn.to(device)\n",
        "Here, a RecSysGNN model with the \"LightGCN\" architecture is initialized. The model's hyperparameters such as latent_dim, num_layers, num_users, and num_items are provided. The model is then moved to the specified device (GPU) using .to(device).\n",
        "\n",
        "Optimizer Setup:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "optimizer = torch.optim.Adam(lightgcn.parameters(), lr=LR)\n",
        "An Adam optimizer is set up to optimize the parameters of the lightgcn model. The learning rate LR is used as defined in your previous code.\n",
        "\n",
        "Print Learnable Embedding Size:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "print(\"Size of Learnable Embedding : \", [x.shape for x in list(lightgcn.parameters())])\n",
        "This code snippet prints the sizes of learnable embeddings in the lightgcn model. In this case, it prints the size of the embedding for users, which is [torch.Size([706, 64])], indicating that there are 706 users and each user's embedding has a dimension of 64.\n",
        "\n",
        "Training and Evaluation:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "light_loss, light_bpr, light_reg, light_recall, light_precision = train_and_eval(lightgcn, optimizer, train_df)\n",
        "The train_and_eval function is called with the lightgcn model, the optimizer, and the training data (train_df). The function returns various loss values (light_loss, light_bpr, light_reg), as well as recall and precision metrics (light_recall, light_precision), which are all assigned to variables for later analysis.\n",
        "\n",
        "Overall, this code snippet initializes, trains, and evaluates a LightGCN-based recommender system model, recording various loss values and evaluation metrics during the training process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Regenerate\n"
      ],
      "metadata": {
        "id": "6kfD744puYZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "This code appears to be responsible for visualizing the training and evaluation results of two different recommender models: LightGCN and NGCF. Let's go through it step by step:\n",
        "\n",
        "Setting Up Epoch List:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "epoch_list = [(i+1) for i in range(EPOCHS)]\n",
        "An epoch_list is created containing the numbers from 1 to EPOCHS, representing the x-axis values for plotting.\n",
        "\n",
        "Plotting LightGCN Training Losses:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "plt.plot(epoch_list, light_loss, label='Total Training Loss')\n",
        "plt.plot(epoch_list, light_bpr, label='BPR Training Loss')\n",
        "plt.plot(epoch_list, light_reg, label='Reg Training Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "This code segment creates a plot with three lines representing different types of loss during the training of the LightGCN model: total training loss, Bayesian Personalized Ranking (BPR) training loss, and regularization training loss. The x-axis corresponds to epochs, and the y-axis corresponds to loss values. Labels and legends are added to differentiate the lines.\n",
        "\n",
        "Plotting LightGCN Recall and Precision Metrics:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "plt.plot(epoch_list, light_recall, label='Recall')\n",
        "plt.plot(epoch_list, light_precision, label='Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metrics')\n",
        "plt.legend()\n",
        "Similar to the previous code segment, this part creates a plot for recall and precision metrics obtained during the training of the LightGCN model. The x-axis represents epochs, and the y-axis represents metric values. Legends are added to differentiate the lines.\n",
        "\n",
        "Creating NGCF Model:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "ngcf = RecSysGNN(\n",
        "  latent_dim=latent_dim,\n",
        "  num_layers=n_layers,\n",
        "  num_users=n_users,\n",
        "  num_items=n_items,\n",
        "  model='NGCF'\n",
        ")\n",
        "ngcf.to(device)\n",
        "An instance of the RecSysGNN model with the \"NGCF\" architecture is created and moved to the specified device (GPU).\n",
        "\n",
        "Optimizer Setup for NGCF:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "optimizer = torch.optim.Adam(ngcf.parameters(), lr=LR)\n",
        "An optimizer is set up to optimize the parameters of the NGCF model.\n",
        "\n",
        "Printing Learnable Embedding Size for NGCF:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "print(\"Size of Learnable Embedding : \", [x.shape for x in list(ngcf.parameters())])\n",
        "This code prints the sizes of learnable embeddings in the NGCF model.\n",
        "\n",
        "Training and Evaluating NGCF:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "ngcf_loss, ngcf_bpr, ngcf_reg, ngcf_recall, ngcf_precision = train_and_eval(ngcf, optimizer, train_df)\n",
        "The NGCF model is trained and evaluated using the train_and_eval function, and the obtained loss and metric values are assigned to the corresponding variables.\n",
        "\n",
        "Plotting NGCF Training Losses:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "plt.plot(epoch_list, ngcf_loss, label='Total Training Loss')\n",
        "plt.plot(epoch_list, ngcf_bpr, label='BPR Training Loss')\n",
        "plt.plot(epoch_list, ngcf_reg, label='Reg Training Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "This section plots the training losses (total, BPR, and regularization) during the training of the NGCF model.\n",
        "\n",
        "Plotting NGCF Recall and Precision Metrics:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "plt.plot(epoch_list, ngcf_recall, label='Recall')\n",
        "plt.plot(epoch_list, ngcf_precision, label='Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metrics')\n",
        "plt.legend()\n",
        "Similar to the previous part, this section creates a plot for recall and precision metrics obtained during the training of the NGCF model.\n",
        "\n",
        "In summary, this code snippet visualizes the training and evaluation results for both the LightGCN and NGCF models. It produces plots to show how the losses and evaluation metrics change over the course of training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Regenerate\n"
      ],
      "metadata": {
        "id": "Mv1CpRIivCVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The code snippet you provided seems to be focused on making movie recommendations using the trained LightGCN and NGCF models. It calculates and displays the recommended movies for a specific user based on their embeddings and the relevance scores of items. Let's go through the code step by step:\n",
        "\n",
        "Calculating Maximum Precision and Recall:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "max(light_precision), max(light_recall)\n",
        "This code calculates and displays the maximum precision and recall values obtained during the training of the LightGCN model.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "max(ngcf_precision), max(ngcf_recall)\n",
        "Similarly, this code calculates and displays the maximum precision and recall values obtained during the training of the NGCF model.\n",
        "\n",
        "Defining the recommend_movies Function:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "def recommend_movies(user_id, user_Embed_wts, item_Embed_wts, n_items, K):\n",
        "    # ... (function implementation)\n",
        "This function takes a user ID, user embeddings, item embeddings, the total number of items, and the number of recommendations to generate as arguments. It returns a list of recommended movie IDs for the specified user.\n",
        "\n",
        "Generating Recommendations for LightGCN and NGCF Models:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "user_id = 122  # Replace with the specific user ID for which you want recommendations\n",
        "recommended_movies_lightgcn = recommend_movies(user_id, lightgcn.embedding.weight, lightgcn.embedding.weight[n_users:], n_items, K)\n",
        "recommended_movies_ngcf = recommend_movies(user_id, ngcf.embedding.weight, ngcf.embedding.weight[n_users:], n_items, K)\n",
        "These lines of code use the recommend_movies function to generate movie recommendations for the specified user ID using both the LightGCN and NGCF models. The recommendations are stored in the variables recommended_movies_lightgcn and recommended_movies_ngcf.\n",
        "\n",
        "Printing Recommended Movies:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "print(\"Recommended movies (LightGCN):\", recommended_movies_lightgcn)\n",
        "print(\"Recommended movies (NGCF):\", recommended_movies_ngcf)\n",
        "These lines print out the recommended movies for the specified user using both the LightGCN and NGCF models.\n",
        "\n",
        "The overall purpose of this code is to demonstrate how to use the trained recommendation models to generate personalized movie recommendations for a specific user and then display those recommendations. The recommend_movies function computes relevance scores for items, sorts them, and returns the top K recommendations. The code snippet then showcases this functionality using the LightGCN and NGCF models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Regenerate\n"
      ],
      "metadata": {
        "id": "8qPO3iorvMkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enhFvEynvcn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the code provided, I can explain the technical process of why movies were recommended to a user and the metrics used to arrive at these recommendations.\n",
        "\n",
        "Generating Movie Recommendations:\n",
        "\n",
        "The process of generating movie recommendations is based on the embeddings of users and items (movies) learned by the recommendation models (LightGCN and NGCF). These embeddings capture the latent features of users and items, allowing the models to understand the preferences and characteristics of both.\n",
        "\n",
        "The recommend_movies function takes the following inputs:\n",
        "\n",
        "user_id: The ID of the user for whom recommendations are being generated.\n",
        "user_Embed_wts: Embeddings of all users.\n",
        "item_Embed_wts: Embeddings of all items (movies).\n",
        "n_items: The total number of items (movies) in the dataset.\n",
        "K: The number of recommendations to generate.\n",
        "Here's how the process works:\n",
        "\n",
        "The function first retrieves the embedding of the specified user using their user_id.\n",
        "It calculates the relevance score between the user's embedding and the embeddings of all items (movies) using matrix multiplication.\n",
        "The items are then ranked based on their relevance scores.\n",
        "The top K items with the highest relevance scores are selected as recommendations for the user.\n",
        "Metrics and Thresholds:\n",
        "\n",
        "From the provided code, the metrics used to evaluate the quality of recommendations are \"precision\" and \"recall.\" These metrics help assess how well the recommended movies match the user's actual interactions or preferences.\n",
        "\n",
        "Precision: This metric measures the proportion of recommended items that the user has actually interacted with or liked. It's calculated as the ratio of the number of recommended items that the user has interacted with to the total number of recommended items. Higher precision indicates that the recommendations are closely aligned with the user's preferences.\n",
        "\n",
        "Recall: This metric measures the proportion of the user's interacted items that were successfully recommended. It's calculated as the ratio of the number of recommended items that the user has interacted with to the total number of items that the user has interacted with. Higher recall indicates that the recommendations are comprehensive and capture a significant portion of the user's preferences.\n",
        "\n",
        "While the exact threshold values for precision and recall are not explicitly mentioned in the provided code, typically there are no fixed \"one-size-fits-all\" threshold values. The choice of threshold values can depend on the specific use case, user behavior, and trade-offs between precision and recall. However, in general, higher values for both precision and recall are desirable.\n",
        "\n",
        "It's important to note that evaluating the quality of recommendations is an ongoing process, and different models and algorithms might have varying precision-recall trade-offs. The chosen metrics and thresholds should align with the specific goals of the recommendation system and the preferences of the users.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fPJfA51Rvdrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, let's delve into the mathematical reasoning behind the metrics and thresholds used to arrive at movie recommendations, specifically precision and recall.\n",
        "\n",
        "Precision:\n",
        "\n",
        "Precision is defined mathematically as the ratio of the number of relevant items retrieved (recommended) to the total number of items retrieved (recommended). In the context of movie recommendations:\n",
        "\n",
        "Precision\n",
        "=\n",
        "Number of Recommended Relevant Movies\n",
        "Total Number of Recommended Movies\n",
        "Precision=\n",
        "Total Number of Recommended Movies\n",
        "Number of Recommended Relevant Movies\n",
        "​\n",
        "\n",
        "\n",
        "The numerator represents the count of movies in the recommendations that the user has actually interacted with (relevant movies).\n",
        "The denominator represents the total number of movies recommended to the user.\n",
        "The higher the precision, the more accurate the recommendations are in terms of aligning with the user's preferences. A precision of 1.0 indicates that all recommended movies are relevant to the user.\n",
        "\n",
        "Recall:\n",
        "\n",
        "Recall is defined mathematically as the ratio of the number of relevant items retrieved (recommended) to the total number of relevant items (those actually interacted with). In the context of movie recommendations:\n",
        "\n",
        "Recall\n",
        "=\n",
        "Number of Recommended Relevant Movies\n",
        "Total Number of Relevant Movies\n",
        "Recall=\n",
        "Total Number of Relevant Movies\n",
        "Number of Recommended Relevant Movies\n",
        "​\n",
        "\n",
        "\n",
        "The numerator represents the count of movies in the recommendations that the user has actually interacted with (relevant movies).\n",
        "The denominator represents the total number of movies that the user has interacted with.\n",
        "The higher the recall, the more comprehensive the recommendations are in terms of capturing the user's preferences. A recall of 1.0 indicates that all the user's interacted movies are included in the recommendations.\n",
        "\n",
        "Thresholds:\n",
        "\n",
        "Thresholds for precision and recall are typically not explicitly defined in the code but rather set by the system designers based on their goals and user behavior. Here's how they can be understood mathematically:\n",
        "\n",
        "Precision Threshold: If you want to ensure that a certain proportion of the recommended movies are relevant, you can set a precision threshold. For example, a precision threshold of 0.8 means you want at least 80% of the recommended movies to be relevant.\n",
        "\n",
        "Recall Threshold: If you want to ensure that a certain proportion of the user's interacted movies are covered by the recommendations, you can set a recall threshold. For example, a recall threshold of 0.7 means you want to capture at least 70% of the user's relevant movies.\n",
        "\n",
        "Adjusting these thresholds allows you to balance the trade-off between precision and recall based on the specific needs of your recommendation system and the preferences of your users. It's important to note that precision and recall are inversely related; as one increases, the other may decrease. The choice of thresholds should consider the user experience and the system's objectives.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Regenerate\n"
      ],
      "metadata": {
        "id": "V5BdXLtmvo7a"
      }
    }
  ]
}